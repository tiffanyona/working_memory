{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IPython magig  tools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"G:/My Drive/WORKING_MEMORY/EXPERIMENTS/ELECTROPHYSIOLOGY/ANALYSIS/src/functions/\")\n",
    "import ephys_functions as ephys\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings. filterwarnings('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "sns.set_context('paper', rc={ \n",
    "                            'xtick.major.size': 1,\n",
    "                            'ytick.major.size': 1,\n",
    "                            'font.family' : 'Arial'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_session_summary(plot, df, colors, variables_combined = ['WM_roll_1', 'RL_roll_1'], \n",
    "                                 y_range = [], x_range = None, epoch = 'Stimulus_ON', baseline=0.5):\n",
    "    \n",
    "    for color, variable, ax in zip(colors, variables_combined, np.repeat(plot, len(variables_combined))):\n",
    "        try:\n",
    "            df_loop = df.loc[(df['trial_type'] == variable)]\n",
    "        except:\n",
    "            df_loop = df\n",
    "\n",
    "        df_loop = df_loop.dropna(axis=1, how='all')\n",
    "\n",
    "        # Select only columns where the column name is a number or can be transformed to a number\n",
    "        numeric_columns = df_loop.columns[df_loop.columns.to_series().apply(pd.to_numeric, errors='coerce').notna()]\n",
    "\n",
    "        real = np.mean(df_loop.groupby('session')[numeric_columns].mean(), axis=0).to_numpy()\n",
    "        times = df_loop[numeric_columns].columns.astype(float)\n",
    "        \n",
    "        df_results = pd.DataFrame()\n",
    "        df_results['times'] = times\n",
    "        df_results['real'] = real\n",
    "        df_results = df_results.sort_values(by='times')\n",
    "\n",
    "        if x_range == None:\n",
    "            x_range = [min(times), max(times)]\n",
    "            \n",
    "        mean_surr = []\n",
    "        df_lower = pd.DataFrame()\n",
    "        df_upper = pd.DataFrame()\n",
    "\n",
    "        df_for_boots = df_loop.groupby('session')\n",
    "        for timepoint in df_results['times'].values:\n",
    "            mean_surr = []\n",
    "\n",
    "            # recover the values for that specific timepoint\n",
    "            try:\n",
    "                array = df_for_boots[timepoint].mean().to_numpy()\n",
    "            except:\n",
    "                array = df_for_boots[str(timepoint)].mean().to_numpy()\n",
    "\n",
    "            # iterate several times with resampling: chose X time among the same list of values\n",
    "            for iteration in range(1000):\n",
    "                x = np.random.choice(array, size=len(array), replace=True)\n",
    "                # recover the mean of that new distribution\n",
    "                mean_surr.append(np.mean(x))\n",
    "\n",
    "            df_lower.at[0, timepoint] = np.percentile(mean_surr, 2.5)\n",
    "            df_upper.at[0, timepoint] = np.percentile(mean_surr, 97.5)\n",
    "\n",
    "        lower =  df_lower.iloc[0].values\n",
    "        upper =  df_upper.iloc[0].values\n",
    "\n",
    "        ax.plot(df_results.times,df_results.real, color=color)\n",
    "        ax.fill_between(df_results.times, lower, upper, alpha=0.2, color=color)\n",
    "        ax.axhline(y=baseline,linestyle=':',color='black')\n",
    "        ax.set_ylim(y_range)\n",
    "        ax.set_xlim(x_range)\n",
    "        ax.set_ylabel('Excess decoding accuracy')\n",
    "        sns.despine()\n",
    "        \n",
    "    if epoch == 'Stimulus_ON':\n",
    "        ax.set_xlabel('Time to stimulus onset (s)')\n",
    "        ax.fill_betweenx(np.arange(-1,1.15,0.1), 0,0.4, color='grey', alpha=.4, edgecolor='none')\n",
    "        ax.fill_betweenx(np.arange(-1,1.15,0.1), 1.4,1.6, color='crimson', alpha=.4, edgecolor='none')\n",
    "        ax.fill_betweenx(np.arange(-1,1.15,0.1), 3.4,3.6, color='green', alpha=.4, edgecolor='none')\n",
    "        ax.fill_betweenx(np.arange(-1,1.15,0.1), 10.4,10.6, color='orange', alpha=.4, edgecolor='none')    \n",
    "    else:\n",
    "        ax.set_xlabel('Time to go cue (s)')\n",
    "        ax.fill_betweenx(np.arange(-1,1.15,0.1), 0,0.2, color='grey', alpha=.4, edgecolor='none')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_session_summary_substract(fig, plot, df: pd.DataFrame, df_shuffle: pd.DataFrame, color, variable= 'WM_roll_1', \n",
    "                                 y_range = [-0.05, 0.3], x_range = None, epoch = 'Stimulus_ON', baseline=0.5):\n",
    "    \n",
    "            # Select only columns where the column name is a number or can be transformed to a number\n",
    "        numeric_columns = df.columns[df.columns.to_series().apply(pd.to_numeric, errors='coerce').notna()]\n",
    "        \n",
    "        df_loop = (df.loc[(df['trial_type'] == variable)].groupby('session').mean()[numeric_columns]\n",
    "                    - df_shuffle.loc[(df['trial_type'] == variable)].groupby('session').mean()[numeric_columns])\n",
    "\n",
    "        real = np.array(np.mean(df_loop.groupby('session').mean()[numeric_columns])) \n",
    "        times = np.array(np.mean(df_loop[numeric_columns]).index).astype(float)\n",
    "\n",
    "        df_results = pd.DataFrame()\n",
    "        df_results['times'] = times\n",
    "        df_results['real'] = real\n",
    "        df_results = df_results.sort_values(by='times')\n",
    "\n",
    "        if x_range == None:\n",
    "            x_range = [min(times), max(times)]\n",
    "            \n",
    "        mean_surr = []\n",
    "        df_lower = pd.DataFrame()\n",
    "        df_upper = pd.DataFrame()\n",
    "\n",
    "        df_for_boots = df_loop.groupby('session').mean()\n",
    "        for timepoint in times:\n",
    "            mean_surr = []\n",
    "\n",
    "            # recover the values for that specific timepoint\n",
    "            try:\n",
    "                array = df_for_boots[timepoint].to_numpy()\n",
    "            except:\n",
    "                array = df_for_boots[str(timepoint)].to_numpy()\n",
    "\n",
    "            # iterate several times with resampling: chose X time among the same list of values\n",
    "            for iteration in range(1000):\n",
    "                x = np.random.choice(array, size=len(array), replace=True)\n",
    "                # recover the mean of that new distribution\n",
    "                mean_surr.append(np.mean(x))\n",
    "\n",
    "            df_lower.at[0, timepoint] = np.percentile(mean_surr, 2.5)\n",
    "            df_upper.at[0, timepoint] = np.percentile(mean_surr, 97.5)\n",
    "\n",
    "        lower =  df_lower.iloc[0].values\n",
    "        upper =  df_upper.iloc[0].values\n",
    "\n",
    "        plot.plot(df_results.times,df_results.real, color=color)\n",
    "        plot.fill_between(df_results.times, lower, upper, alpha=0.2, color=color)\n",
    "        plot.axhline(y=baseline,linestyle=':',color='black')\n",
    "        plot.set_ylim(y_range)\n",
    "        plot.set_xlim(x_range)\n",
    "\n",
    "        if epoch == 'Stimulus_ON':\n",
    "            plot.set_xlabel('Time to stimulus onset (s)')\n",
    "            plot.fill_betweenx(np.arange(-1,1.15,0.1), 0,0.4, color='grey', alpha=.4)\n",
    "        else:\n",
    "            plot.set_xlabel('Time to go cue (s)')\n",
    "            plot.fill_betweenx(np.arange(-1,1.15,0.1), 0,0.2, color='grey', alpha=.4)\n",
    "\n",
    "        sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "## Delay specific decoder\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r'C:\\Users\\tiffany.ona\\Documents\\Ephys\\summary_complete'\n",
    "save_data = r'C:\\Users\\tiffany.ona\\Documents\\working_memory\\data\\1_5_Review_Decoder across delays trained on 3s or 10s delays'\n",
    "save_figures = r'G:\\My Drive\\WORKING_MEMORY\\PAPER\\2ND_SUBMISSION_NAT_NEURO\\figures_for_resubmission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interval_extraction_trial(df, cluster_list=[], variable = 'vector_answer', align = 'Delay_OFF', start = 0, stop = 1, delay_only=False):\n",
    "    y = []\n",
    "    d = {}\n",
    "    \n",
    "    if delay_only == False:\n",
    "        # print('Skipping delays')\n",
    "        if align == 'Delay_OFF' and start < 0:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2)]\n",
    "        if align == 'Delay_OFF' and start < -1:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2) & (df.delay != 1)]\n",
    "\n",
    "        if align == 'Stimulus_ON' and stop > 0.5:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2)]\n",
    "\n",
    "        if align == 'Stimulus_ON' and stop > 1.5:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2) & (df.delay != 1)]\n",
    "    \n",
    "    # print('Recovered from: ', str(len(df.trial.unique())), ' trials')\n",
    "    # Create new aligment to the end of the session\n",
    "    df['a_'+align] = df.fixed_times-df[align]\n",
    "\n",
    "    # cluster_list = df_all.cluster_id.unique()\n",
    "    df = df.sort_values('trial')\n",
    "    \n",
    "    y = df.groupby('trial')[variable].mean()\n",
    "\n",
    "    # Filter for the spikes that occur in the interval we are analyzing\n",
    "    df = df.loc[(df['a_'+align]>start)&(df['a_'+align]<stop)]\n",
    "\n",
    "    df_final = pd.DataFrame()\n",
    "    df_final = df.groupby(['trial','cluster_id']).count()\n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.pivot_table(index=['trial'], columns='cluster_id', values='fixed_times', fill_value=0).rename_axis(None, axis=1)\n",
    "    df_final = df_final.reindex(cluster_list, axis=1,fill_value=0)\n",
    "\n",
    "    result = pd.merge(df_final, y, how=\"right\", on=[\"trial\"]).fillna(0)\n",
    "    result = result.rename(columns={variable: \"y\"})\n",
    "    result['y'] = np.where(result['y'] == 0, -1, result['y']) \n",
    "    \n",
    "    return result, result['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(df, decode='vector_answer', align='Delay_OFF', start=-0.5, stop=0, cluster_list = [], ratio=0.65, test_index=[],  train_index=[], fakey=[], delay_only=False):\n",
    "\n",
    "    df_final, y = interval_extraction_trial(df,variable = decode, align = 'Stimulus_ON', start = -0.25, stop = 0, cluster_list = cluster_list, delay_only=delay_only)\n",
    "    \n",
    "    sc = RobustScaler()\n",
    "    x = df_final.iloc[:, df_final.columns != 'y']\n",
    "    sc_fit = sc.fit(x)\n",
    "    \n",
    "    df_final, y = interval_extraction_trial(df,variable = decode, align = align, start = start, stop = stop, cluster_list = cluster_list, delay_only=delay_only)\n",
    "    \n",
    "    # This is mainly for the session shuffles\n",
    "    if len(fakey) > 1:\n",
    "        print('Using shuffled session')\n",
    "        y = fakey[len(fakey)-len(y):]\n",
    "        df_final['y'] = y   \n",
    "        \n",
    "    train_cols = df_final.columns\n",
    "    \n",
    "    #Train the model   \n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.drop(columns ='trial')\n",
    "    \n",
    "    if len(test_index) >= 1:\n",
    "        print('Using splits')\n",
    "        train = df_final.loc[train_index,:]\n",
    "        test = df_final.loc[test_index,:]\n",
    "        # print('Fold',str(fold_no),'Class Ratio:',sum(test['y'])/len(test['y']))\n",
    "        x_test = test.iloc[:, test.columns != 'y']\n",
    "        y_test = test['y']\n",
    "        x_train = train.iloc[:, train.columns != 'y']\n",
    "        y_train = train['y']\n",
    "        \n",
    "    else:\n",
    "        x_train = df_final.iloc[:, df_final.columns != 'y']\n",
    "        y_train = df_final['y']\n",
    "        x_test = x_train\n",
    "        y_test = y_train\n",
    "        \n",
    "    #Normalize the X data\n",
    "    # sc = RobustScaler()\n",
    "    # sc_fit = sc.fit(df_final.iloc[:, df_final.columns != 'y'])\n",
    "    \n",
    "    # x_train = sc.fit_transform(x_train)\n",
    "    # x_test = sc.fit_transform(x_test)\n",
    "    \n",
    "    # x_train = sc_fit.transform(x_train)\n",
    "    # x_test = sc_fit.transform(x_test)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', class_weight='balanced').fit(x_train, y_train)\n",
    "    # model = LogisticRegression(solver='liblinear', penalty = 'l1', C=0.95).fit(x_train, y_train)\n",
    "    train_cols = df_final.columns\n",
    "    \n",
    "    p_pred = model.predict_proba(x_test)    \n",
    "    y_pred = model.predict(x_test)    \n",
    "    f1score= f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    y_test = np.where(y_test == -1, 0, y_test) \n",
    "    y_new = y_test.reshape(len(y_test), 1).astype(int)\n",
    "    # y_new = y_test.values.reshape(len(y_test), 1).astype(int)\n",
    "    score_ =  np.take_along_axis(p_pred,y_new,axis=1)   \n",
    "\n",
    "    # print('Trained model on ', len(train_cols), ' neurons.')\n",
    "    print('score:', np.mean(score_), 'f1_score ', f1score)\n",
    "    \n",
    "    return model, train_cols, np.mean(score_), sc_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(df,sc_fit, epoch='Stimulus_ON',initrange=-0.4,endrange=1.5,r=0.2, model = None, train_cols=None, variable='ra_accuracy',\n",
    "                      hit=1, nsurrogates = 100, decode='vector_answer', ratio=0, cluster_list = [], test_index=[], fakey=[], delay_only=False):\n",
    "    '''\n",
    "    Function that tests a previously trained function (func. train_decoder) on population activity of specific segments\n",
    "    \n",
    "    Attributes\n",
    "        - df: DataFrame. it contains a whole ephys session without curation. \n",
    "        - WM and RL are the variables to consider a trial in the RL or in the WM-module. Both need to be floats. \n",
    "        - epoch: str. Moment at which the data will be aligned to. \n",
    "        - initrange: float. \n",
    "        - endrange: float.\n",
    "        - r: float \n",
    "        - model. function. \n",
    "        - train_cols\n",
    "        - name. String\n",
    "        - variables. List. \n",
    "        - hits. List. \n",
    "        - colors. List\n",
    "        - nsurrogates. Int. \n",
    "        - indexes. List \n",
    "        - decode. String\n",
    "    \n",
    "    Return\n",
    "        - df_real\n",
    "        - df_iter\n",
    "        It will also make a plot. \n",
    "    '''\n",
    "    \n",
    "    df_real = pd.DataFrame()\n",
    "    df_iter = pd.DataFrame(columns=['iteration', 'score', 'times', 'epoch', 'variable'])\n",
    "        \n",
    "    times = [] # Timestamps\n",
    "    real_score = [] # real scoring of the decoded\n",
    "    mean_sur=[] # mean of the surrogate data\n",
    "\n",
    "    for start, stop in zip(np.arange(initrange,endrange-r,r/2),np.arange(initrange+r,endrange,r/2)):\n",
    "        times.append((start+stop)/2)\n",
    "        df_final, y = interval_extraction_trial(df,variable = decode, align = epoch, start = start, stop = stop, cluster_list=cluster_list, delay_only=delay_only)\n",
    "\n",
    "        # Sometimes the testing and the trainind dataset have different neurons since they are looking at different trials and perhaps there were no spikes\n",
    "        # coming from all neurons. We compare which columns are missing and add them containing 0 for the model to work. \n",
    "        test_cols = df_final.columns\n",
    "        common_cols = train_cols.intersection(test_cols)\n",
    "        train_not_test = train_cols.difference(test_cols)\n",
    "        for col in train_not_test:\n",
    "            df_final[col] = 0\n",
    "\n",
    "        #The other way round. When training in segmented data, sometimes the training set is smaller than the testing (for instance, when training in Hb trials and testing in WM)\n",
    "        test_not_train = test_cols.difference(train_cols)\n",
    "        for col in test_not_train:\n",
    "            df_final.drop(columns=[col],inplace=True)\n",
    "\n",
    "        #Train the model\"\n",
    "        if len(test_index) >= 1:\n",
    "            print('Train splitting trials')\n",
    "            # Split data in training and testing\n",
    "            # x_train, x_test, y_train, y_test =\\\n",
    "            #     train_test_split(df_final, y, test_size=test_sample,random_state=random_state)\n",
    "            \n",
    "            df_final.reset_index(inplace=True)\n",
    "            df_final = df_final.drop(columns ='trial')\n",
    "            test = df_final.loc[test_index,:]\n",
    "            # print('Fold',str(fold_no),'Class Ratio:',sum(test['y'])/len(test['y']))\n",
    "            x_test = test.iloc[:, test.columns != 'y']\n",
    "            y_test = test['y']             \n",
    "\n",
    "        else:\n",
    "            x_train = df_final.iloc[:, df_final.columns != 'y']\n",
    "            y_train = df_final['y']\n",
    "            x_test = x_train\n",
    "            y_test = y_train\n",
    "        \n",
    "        #Normalize the X data\n",
    "        # sc = RobustScaler()\n",
    "        # x_test = sc.fit_transform(x_test)\n",
    "        # x_test = sc_fit.transform(x_test)\n",
    "        \n",
    "        p_pred = model.predict_proba(x_test)\n",
    "        y_pred = model.predict(x_test)\n",
    "        score_ = model.score(x_test, y_test)\n",
    "        real_score.append(score_)\n",
    "\n",
    "        # y_test = np.where(y_test == -1, 0, y_test) \n",
    "        # y_new = y_test.reshape(len(y_test), 1).astype(int)\n",
    "        # corrected_score =  np.take_along_axis(p_pred,y_new,axis=1)   \n",
    "        # real_score.append(np.mean(corrected_score))\n",
    "\n",
    "        # print('score:', score_, 'corrected score: ', np.mean(corrected_score), end='\\n\\n')\n",
    "\n",
    "        i=0\n",
    "        rows = []\n",
    "        while i <= nsurrogates:\n",
    "            i+=1\n",
    "            y_perr = shuffle(y_test)\n",
    "            score_ = model.score(x_test, y_perr)\n",
    "\n",
    "            # y_new = y_perr.reshape(len(y_perr), 1).astype(int)\n",
    "            # result =  np.take_along_axis(p_pred,y_new,axis=1)     \n",
    "            # score_  = np.mean(result)\n",
    "\n",
    "            new_row = pd.DataFrame({'iteration': [i], 'score': [score_], 'times': [(start+stop)/2], 'epoch' : [epoch], \n",
    "                                      'variable' : [variable+'_'+str(hit)]})\n",
    "            df_iter = pd.concat([df_iter,new_row], ignore_index=True)\n",
    "        \n",
    "    times.append('trial_type')\n",
    "    real_score.append(variable+'_'+str(hit))\n",
    "    a_series = pd.DataFrame([real_score], columns = times)\n",
    "\n",
    "    df_real = pd.concat([df_real,a_series], ignore_index=True)\n",
    "    \n",
    "    return df_real, df_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "os.chdir(path_to_data)\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "# for filename in list_of_sessions:\n",
    "    # \n",
    "    if filename[-3:] != 'pdf':\n",
    "        df = pd.read_csv(filename, sep=',',index_col=0)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    print(filename, '/ Total session trials: ', len(df.trial.unique()), '/ Number of neurons: ', len(df.cluster_id.unique()))\n",
    "    \n",
    "    df = ephys.add_time_before_stimulus(df, 4)\n",
    "\n",
    "    substract = True\n",
    "    df['delay'] = np.around(df.delay,2)\n",
    "    # Variables used for decoder training\n",
    "    decode = 'vector_answer'\n",
    "    align='Delay_OFF'\n",
    "    ratio = 0.6\n",
    "    delay_train = 'all'\n",
    "    start = -0.5\n",
    "    stop = 0\n",
    "    type_trial ='WM_roll'\n",
    "    hit = 1\n",
    "    nsplits = 5\n",
    "    \n",
    "    cluster_list = df.cluster_id.unique()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=nsplits, shuffle=True)\n",
    "    # skf = KFold(n_splits=nsplits, shuffle=True)\n",
    "\n",
    "    # Create a dataframe for training data\n",
    "    if type_trial == 'all':\n",
    "        df_train = df.loc[(df.hit==hit)]\n",
    "    elif hit == 'all':\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)]\n",
    "    else:\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)&(df.hit==hit)]\n",
    "\n",
    "    if delay_train != 'all':\n",
    "        df_train = df_train.loc[(df_train.delay==delay_train)]\n",
    "    else:\n",
    "        df_train = df_train.loc[(df_train.delay!=0.2)&(df_train.delay!=0.1)]\n",
    "        \n",
    "    df_final, y = interval_extraction_trial(df_train, variable = decode, align = align, start = start, stop = stop, cluster_list=cluster_list)\n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.drop(columns ='trial')\n",
    "    df_final = df_final.drop(columns ='y')\n",
    "\n",
    "    # Assuming df is your DataFrame with neural spike data\n",
    "    # Replace 'features' and 'target' with the actual column names\n",
    "    X = df_final  # Features (neural spike data)\n",
    "    y = y  # Target variable\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define a pipeline for scaling and logistic regression\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('logreg', LogisticRegression(max_iter=500))  # Logistic Regression\n",
    "    ])\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'logreg__solver': ['liblinear', 'saga'],  # Solvers to test\n",
    "        'logreg__penalty': ['l1', 'l2'],          # Regularization types\n",
    "        'logreg__C': [0.01, 0.1, 1, 10, 100]     # Regularization strength\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and corresponding score\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Cross-Validated Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    print(\"Test Set Accuracy:\", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_________________________________________\n",
    "#### **Train in one delay and test in other delays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dataframe used for cumulative analysis\n",
    "df_cum = pd.DataFrame()\n",
    "df_cum_shuffle = pd.DataFrame()\n",
    "\n",
    "os.chdir(path_to_data)\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "# for filename in list_of_sessions:\n",
    "    # \n",
    "    if filename[-3:] != 'pdf':\n",
    "        df = pd.read_csv(filename, sep=',',index_col=0)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    print(filename, '/ Total session trials: ', len(df.trial.unique()), '/ Number of neurons: ', len(df.cluster_id.unique()))\n",
    "    \n",
    "    df = ephys.add_time_before_stimulus(df, -4)\n",
    "\n",
    "    substract = True\n",
    "    df['delay'] = np.around(df.delay,2)\n",
    "    \n",
    "    if 10 not in df.delay.unique():\n",
    "        print('Skip session because no 10s delay')\n",
    "        continue\n",
    "    \n",
    "    # Variables used for decoder training\n",
    "    decode = 'vector_answer'\n",
    "    align='Delay_OFF'\n",
    "    ratio = 0.6\n",
    "    delay_train = 3\n",
    "    start = -1\n",
    "    stop = 0\n",
    "    type_trial ='WM_roll'\n",
    "    hit = 1\n",
    "    nsplits = 5\n",
    "    \n",
    "    #Variables for testing\n",
    "    colors=['darkgreen','darkorange', 'crimson']\n",
    "    variables = ['WM_roll','WM_roll','WM_roll']\n",
    "    hits = [1,1,1]\n",
    "    ratios = [0.6,0.6,0.6]\n",
    "    delays = [1, 3, 10]\n",
    "    variables_combined = [f\"{variables[i]}_{delays[i]}\" for i in range(len(variables))]\n",
    "            \n",
    "    cluster_list = df.cluster_id.unique()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=nsplits, shuffle=True)\n",
    "\n",
    "    # Create a dataframe for training data\n",
    "    if type_trial == 'all':\n",
    "        df_train = df.loc[(df.hit==hit)]\n",
    "    elif hit == 'all':\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)]\n",
    "    else:\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)&(df.hit==hit)]\n",
    "\n",
    "    # print('Total of delay 3s: ', df_train.loc[df_train['delay'] == 3].trial.nunique(), '; Total of 10s: ', df_train.loc[df_train['delay'] == 10].trial.nunique())\n",
    "    \n",
    "    if delay_train != 'all':\n",
    "        df_train = df_train.loc[(df_train.delay==delay_train)]\n",
    "    else:\n",
    "        df_train = df_train.loc[(df_train.delay!=0.2)&(df_train.delay!=0.1)]\n",
    "            \n",
    "    df_final, y = interval_extraction_trial(df_train, variable = decode, align = align, start = start, stop = stop, cluster_list=cluster_list)\n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.drop(columns ='trial')\n",
    "\n",
    "    fold_no = 1\n",
    "    if len(y) < nsplits:\n",
    "        print('Skip session because not enough trials')\n",
    "        continue\n",
    "        \n",
    "    for train_index, test_index in skf.split(df_final, y):\n",
    "        # print('Fold_no:', fold_no)\n",
    "        model, train_cols, score, sc_fit = train(df_train, decode=decode, align=align, start=start,stop=stop, cluster_list = cluster_list, \n",
    "                                  test_index=test_index,  train_index=train_index)\n",
    "\n",
    "        # Remove a fifth of the dataset so it can be compared to crossvalidated data. If we want to randomly reduce it, add reduce to trian function\n",
    "        # drop_list = np.array_split(df_train.trial.unique(), 5)[fold_no]\n",
    "        # df_train = df_train[~df_train['trial'].isin(drop_list)]\n",
    "        # index_train_trials = df_train.trial.unique()\n",
    "        # print('Total of left: ', len(df_train.loc[df_train['vector_answer'] == 0].groupby('trial').mean()), '; Total of right: ', len(df_train.loc[df_train['vector_answer'] == 1].groupby('trial').mean()))\n",
    "\n",
    "        for color, variable, delay, hit, ratio in zip(colors,variables, delays, hits,ratios):\n",
    "            df_res = pd.DataFrame()\n",
    "            df_sti = pd.DataFrame()\n",
    "            df_iter = pd.DataFrame()\n",
    "            try:\n",
    "                df_delay = df.loc[np.around(df.delay,1)==delay]\n",
    "                delay=np.around(df_delay.delay.iloc[0],1)\n",
    "                # print('Delay:', delay)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if delay == 0.1 or delay == 0.2:\n",
    "                endrange=3.5\n",
    "                r=0.25\n",
    "\n",
    "            elif delay == 1:\n",
    "                endrange=4.5\n",
    "                r=0.25\n",
    "\n",
    "            elif delay == 3:\n",
    "                endrange=6.5    \n",
    "                r=0.25\n",
    "                \n",
    "            elif delay == 10:\n",
    "                endrange=14.5\n",
    "                r=0.25\n",
    "            \n",
    "            # Create a dataframe for testing data\n",
    "            if variable == 'all':\n",
    "                df_test = df_delay.loc[(df_delay.hit==hit)]\n",
    "            elif hit == 'all':\n",
    "                df_test = df_delay.loc[(df_delay[variable]>=ratio)]\n",
    "            else:\n",
    "                df_test = df_delay.loc[(df_delay[variable]>=ratio)&(df_delay.hit==hit)]\n",
    "                \n",
    "            if fold_no == 1:\n",
    "                print(delay, variable, 'Threshold:', ratio, 'Hit:', hit, 'Nº of trials:', len(df_test.trial.unique()))\n",
    "\n",
    "# -----------  Remove the trials that overlap with the training set.\n",
    "            list_train_trials = df_train.trial.unique()[train_index]\n",
    "            df_test = df_test[~df_test['trial'].isin(list_train_trials)] \n",
    "            \n",
    "            if delay_train != delay:\n",
    "                # print('Reduced trials from:', len(df_test.trial.unique()))\n",
    "                values = df_test.groupby('trial').mean().reset_index()\n",
    "                select_trials = values.sample(n=len(test_index), replace=True).trial.unique()\n",
    "                df_test = df_test[df_test['trial'].isin(select_trials)]\n",
    "                # print('Reduced trials to:', len(df_test.trial.unique()))\n",
    "                \n",
    "            if len(df_test.trial.unique()) < 5:\n",
    "                print('Not enough trials with this condition')\n",
    "                continue\n",
    "\n",
    "            df_real,df_temp = test(df_test, sc_fit, decode= decode,epoch='Stimulus_ON',initrange=-2,endrange=endrange, r=r, model = model, delay_only=delay, variable=variable, hit=hit, nsurrogates = 100,train_cols = train_cols, cluster_list = cluster_list)\n",
    "\n",
    "            df_sti = pd.concat([df_real,df_sti])\n",
    "            df_iter = pd.concat([df_iter,df_temp])\n",
    "            \n",
    "            variable = str(variable)+'_'+str(hit)\n",
    "\n",
    "            # Aligmnent for Stimulus cue\n",
    "            real = df_sti.loc[(df_sti['trial_type'] ==variable)].to_numpy()\n",
    "            times = np.around(np.array(df_sti.columns)[:-1].astype(float),2)\n",
    "\n",
    "            df_new= df_iter.loc[(df_iter.epoch=='Stimulus_ON')].groupby('times')['score']\n",
    "            y_mean= df_new.mean().values\n",
    "            lower =  df_new.quantile(q=0.975, interpolation='linear') - y_mean\n",
    "            upper =  df_new.quantile(q=0.025, interpolation='linear') - y_mean\n",
    "            x=times\n",
    "            \n",
    "            # ephys.plot_results_decoder(fig, real[0][:len(y_mean)], times, df_new,  ax1, color = color, epoch = 'Delay_OFF', \n",
    "            #             y_range = [-0.05, 0.5], x_range = None, substract=True)\n",
    "            \n",
    "            df_cum, df_cum_shuffle = ephys.add_to_summary(real[0][:len(y_mean)], y_mean, times, filename, variable, epoch = 'Stimulus_ON', fold_no=fold_no, df_iter=df_iter, df_cum=df_cum, df_cum_shuffle=df_cum_shuffle, substract=True, delay=delay)\n",
    "            \n",
    "            sns.despine()\n",
    "            \n",
    "        fold_no+=1\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = 'G:/My Drive/WORKING_MEMORY/PAPER/ANALYSIS_figures/'\n",
    "# os.chdir(save_path)\n",
    "\n",
    "df_cum['trial_type'] = np.where(df_cum['delay'] == 3, 'WM_3', df_cum['trial_type'])\n",
    "df_cum['trial_type'] = np.where(df_cum['delay'] == 10, 'WM_10', df_cum['trial_type'])\n",
    "df_cum['trial_type'] = np.where(df_cum['delay'] == 1, 'WM_1', df_cum['trial_type'])\n",
    "\n",
    "# file_name = f'/review_crossdelay_decoder_train_{delay_train}.csv'\n",
    "# df_cum.to_csv(save_data+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10, 4), sharey=True)\n",
    "baseline = 0\n",
    "plot_results_session_summary(ax, df_cum, colors, variables_combined = ['WM_3', 'WM_10', 'WM_1'], \n",
    "                                y_range = [baseline-0.05, baseline+0.3], x_range = [-2,13], epoch = 'Stimulus_ON', baseline=baseline)\n",
    "\n",
    "# fig.savefig(save_figures+f'/review_decoder_across_delays_train_{train_delay}s.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review figure and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_train =10\n",
    "file_name = f'/review_crossdelay_decoder_train_{delay_train}.csv'\n",
    "df_cum = pd.read_csv(save_data+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables for testing\n",
    "colors=['darkgreen','darkorange', 'crimson']\n",
    "baseline = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 4), sharey=True)\n",
    "baseline = 0\n",
    "plot_results_session_summary(ax, df_cum, colors, variables_combined = ['WM_3', 'WM_10', 'WM_1'], \n",
    "                                y_range = [baseline-0.05, baseline+0.3], x_range = [-2,13], epoch = 'Stimulus_ON', baseline=baseline)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "fig.savefig(save_figures+f'/review_decoder_across_delays_train_{delay_train}s.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
