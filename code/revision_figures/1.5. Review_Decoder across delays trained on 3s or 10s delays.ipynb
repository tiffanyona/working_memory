{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "## Delay specific decoder\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = r'C:\\Users\\tiffany.ona\\Documents\\Ephys\\summary_complete'\n",
    "save_data = r'G:\\My Drive\\WORKING_MEMORY\\PAPER\\2ND_SUBMISSION_NAT_NEURO\\data_for_resubmission'\n",
    "save_figures = r'G:\\My Drive\\WORKING_MEMORY\\PAPER\\2ND_SUBMISSION_NAT_NEURO\\figures_for_resubmission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IPython magig  tools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"G:/My Drive/WORKING_MEMORY/EXPERIMENTS/ELECTROPHYSIOLOGY/ANALYSIS/src/functions/\")\n",
    "import ephys_functions as ephys\n",
    "import model_functions as mod\n",
    "import behavioral_functions as beh\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings. filterwarnings('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interval_extraction_trial(df, cluster_list=[], variable = 'vector_answer', align = 'Delay_OFF', start = 0, stop = 1, delay_only=False):\n",
    "    y = []\n",
    "    d = {}\n",
    "    \n",
    "    if delay_only == False:\n",
    "        # print('Skipping delays')\n",
    "        if align == 'Delay_OFF' and start < 0:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2)]\n",
    "        if align == 'Delay_OFF' and start < -1:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2) & (df.delay != 1)]\n",
    "\n",
    "        if align == 'Stimulus_ON' and stop > 0.5:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2)]\n",
    "\n",
    "        if align == 'Stimulus_ON' and stop > 1.5:\n",
    "            df = df.loc[(df.delay != 0.1) & (df.delay != 0.2) & (df.delay != 1)]\n",
    "    \n",
    "    # print('Recovered from: ', str(len(df.trial.unique())), ' trials')\n",
    "    # Create new aligment to the end of the session\n",
    "    df['a_'+align] = df.fixed_times-df[align]\n",
    "\n",
    "    # cluster_list = df_all.cluster_id.unique()\n",
    "    df = df.sort_values('trial')\n",
    "    \n",
    "    y = df.groupby('trial')[variable].mean()\n",
    "\n",
    "    # Filter for the spikes that occur in the interval we are analyzing\n",
    "    df = df.loc[(df['a_'+align]>start)&(df['a_'+align]<stop)]\n",
    "\n",
    "    df_final = pd.DataFrame()\n",
    "    df_final = df.groupby(['trial','cluster_id']).count()\n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.pivot_table(index=['trial'], columns='cluster_id', values='fixed_times', fill_value=0).rename_axis(None, axis=1)\n",
    "    df_final = df_final.reindex(cluster_list, axis=1,fill_value=0)\n",
    "\n",
    "    result = pd.merge(df_final, y, how=\"right\", on=[\"trial\"]).fillna(0)\n",
    "    result = result.rename(columns={variable: \"y\"})\n",
    "    result['y'] = np.where(result['y'] == 0, -1, result['y']) \n",
    "    \n",
    "    return result, result['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(df, decode='vector_answer', align='Delay_OFF', start=-0.5, stop=0, cluster_list = [], ratio=0.65, test_index=[],  train_index=[], fakey=[], delay_only=False):\n",
    "\n",
    "    df_final, y = interval_extraction_trial(df,variable = decode, align = 'Stimulus_ON', start = -0.25, stop = 0, cluster_list = cluster_list, delay_only=delay_only)\n",
    "    sc = RobustScaler()\n",
    "    x =df_final.iloc[:, df_final.columns != 'y']\n",
    "    sc_fit = sc.fit(x)\n",
    "    \n",
    "    df_final, y = interval_extraction_trial(df,variable = decode, align = align, start = start, stop = stop, cluster_list = cluster_list, delay_only=delay_only)\n",
    "    \n",
    "    # This is mainly for the session shuffles\n",
    "    if len(fakey) > 1:\n",
    "        print('Using shuffled session')\n",
    "        y = fakey[len(fakey)-len(y):]\n",
    "        df_final['y'] = y   \n",
    "        \n",
    "    train_cols = df_final.columns\n",
    "    \n",
    "    #Train the model   \n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.drop(columns ='trial')\n",
    "    \n",
    "    if len(test_index) >= 1:\n",
    "        print('Using splits')\n",
    "        train = df_final.loc[train_index,:]\n",
    "        test = df_final.loc[test_index,:]\n",
    "        # print('Fold',str(fold_no),'Class Ratio:',sum(test['y'])/len(test['y']))\n",
    "        x_test = test.iloc[:, test.columns != 'y']\n",
    "        y_test = test['y']\n",
    "        x_train = train.iloc[:, train.columns != 'y']\n",
    "        y_train = train['y']\n",
    "        \n",
    "    else:\n",
    "        x_train = df_final.iloc[:, df_final.columns != 'y']\n",
    "        y_train = df_final['y']\n",
    "        x_test = x_train\n",
    "        y_test = y_train\n",
    "        \n",
    "    #Normalize the X data\n",
    "    sc = RobustScaler()\n",
    "    sc_fit = sc.fit(df_final.iloc[:, df_final.columns != 'y'])\n",
    "    \n",
    "    # x_train = sc.fit_transform(x_train)\n",
    "    # x_test = sc.fit_transform(x_test)\n",
    "    \n",
    "    x_train = sc_fit.transform(x_train)\n",
    "    x_test = sc_fit.transform(x_test)\n",
    "    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_grid = {\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10]\n",
    "    }\n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', penalty = 'l1').fit(x_train, y_train)\n",
    "    # model = LogisticRegression(solver='liblinear', penalty = 'l1', C=0.95, fit_intercept=True).fit(x_train, y_train)\n",
    "    train_cols = df_final.columns\n",
    "    \n",
    "    p_pred = model.predict_proba(x_test)    \n",
    "    y_pred = model.predict(x_test)    \n",
    "    f1score= f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    y_test = np.where(y_test == -1, 0, y_test) \n",
    "    y_new = y_test.reshape(len(y_test), 1).astype(int)\n",
    "    # y_new = y_test.values.reshape(len(y_test), 1).astype(int)\n",
    "    score_ =  np.take_along_axis(p_pred,y_new,axis=1)   \n",
    "\n",
    "    # print('Trained model on ', len(train_cols), ' neurons.')\n",
    "    print('score:', np.mean(score_), 'f1_score ', f1score)\n",
    "    \n",
    "    return model, train_cols, np.mean(score_), sc_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(df,sc_fit, epoch='Stimulus_ON',initrange=-0.4,endrange=1.5,r=0.2, model = None, train_cols=None, variable='ra_accuracy',\n",
    "                      hit=1, nsurrogates = 100, decode='vector_answer', ratio=0, cluster_list = [], test_index=[], fakey=[], delay_only=False):\n",
    "    '''\n",
    "    Function that tests a previously trained function (func. train_decoder) on population activity of specific segments\n",
    "    \n",
    "    Attributes\n",
    "        - df: DataFrame. it contains a whole ephys session without curation. \n",
    "        - WM and RL are the variables to consider a trial in the RL or in the WM-module. Both need to be floats. \n",
    "        - epoch: str. Moment at which the data will be aligned to. \n",
    "        - initrange: float. \n",
    "        - endrange: float.\n",
    "        - r: float \n",
    "        - model. function. \n",
    "        - train_cols\n",
    "        - name. String\n",
    "        - variables. List. \n",
    "        - hits. List. \n",
    "        - colors. List\n",
    "        - nsurrogates. Int. \n",
    "        - indexes. List \n",
    "        - decode. String\n",
    "    \n",
    "    Return\n",
    "        - df_real\n",
    "        - df_iter\n",
    "        It will also make a plot. \n",
    "    '''\n",
    "    \n",
    "    df_real = pd.DataFrame()\n",
    "    df_iter = pd.DataFrame(columns=['iteration', 'score', 'times', 'epoch', 'variable'])\n",
    "        \n",
    "    times = [] # Timestamps\n",
    "    real_score = [] # real scoring of the decoded\n",
    "    mean_sur=[] # mean of the surrogate data\n",
    "\n",
    "    for start, stop in zip(np.arange(initrange,endrange-r,r/2),np.arange(initrange+r,endrange,r/2)):\n",
    "        times.append((start+stop)/2)\n",
    "        df_final, y = interval_extraction_trial(df,variable = decode, align = epoch, start = start, stop = stop, cluster_list=cluster_list, delay_only=delay_only)\n",
    "\n",
    "        # Sometimes the testing and the trainind dataset have different neurons since they are looking at different trials and perhaps there were no spikes\n",
    "        # coming from all neurons. We compare which columns are missing and add them containing 0 for the model to work. \n",
    "        test_cols = df_final.columns\n",
    "        common_cols = train_cols.intersection(test_cols)\n",
    "        train_not_test = train_cols.difference(test_cols)\n",
    "        for col in train_not_test:\n",
    "            df_final[col] = 0\n",
    "\n",
    "        #The other way round. When training in segmented data, sometimes the training set is smaller than the testing (for instance, when training in Hb trials and testing in WM)\n",
    "        test_not_train = test_cols.difference(train_cols)\n",
    "        for col in test_not_train:\n",
    "            df_final.drop(columns=[col],inplace=True)\n",
    "\n",
    "        #Train the model\"\n",
    "        if len(test_index) >= 1:\n",
    "            print('Train splitting trials')\n",
    "            # Split data in training and testing\n",
    "            # x_train, x_test, y_train, y_test =\\\n",
    "            #     train_test_split(df_final, y, test_size=test_sample,random_state=random_state)\n",
    "            \n",
    "            df_final.reset_index(inplace=True)\n",
    "            df_final = df_final.drop(columns ='trial')\n",
    "            test = df_final.loc[test_index,:]\n",
    "            # print('Fold',str(fold_no),'Class Ratio:',sum(test['y'])/len(test['y']))\n",
    "            x_test = test.iloc[:, test.columns != 'y']\n",
    "            y_test = test['y']             \n",
    "\n",
    "        else:\n",
    "            x_train = df_final.iloc[:, df_final.columns != 'y']\n",
    "            y_train = df_final['y']\n",
    "            x_test = x_train\n",
    "            y_test = y_train\n",
    "        \n",
    "        #Normalize the X data\n",
    "        # sc = RobustScaler()\n",
    "        # x_test = sc.fit_transform(x_test)\n",
    "        # x_test = sc_fit.transform(x_test)\n",
    "        \n",
    "        p_pred = model.predict_proba(x_test)\n",
    "        y_pred = model.predict(x_test)\n",
    "        score_ = model.score(x_test, y_test)\n",
    "        real_score.append(score_)\n",
    "\n",
    "        # y_test = np.where(y_test == -1, 0, y_test) \n",
    "        # y_new = y_test.reshape(len(y_test), 1).astype(int)\n",
    "        # corrected_score =  np.take_along_axis(p_pred,y_new,axis=1)   \n",
    "        # real_score.append(np.mean(corrected_score))\n",
    "\n",
    "        # print('score:', score_, 'corrected score: ', np.mean(corrected_score), end='\\n\\n')\n",
    "\n",
    "        i=0\n",
    "        rows = []\n",
    "        while i <= nsurrogates:\n",
    "            i+=1\n",
    "            y_perr = shuffle(y_test)\n",
    "            score_ = model.score(x_test, y_perr)\n",
    "\n",
    "            # y_new = y_perr.reshape(len(y_perr), 1).astype(int)\n",
    "            # result =  np.take_along_axis(p_pred,y_new,axis=1)     \n",
    "            # score_  = np.mean(result)\n",
    "\n",
    "            new_row = pd.DataFrame({'iteration': [i], 'score': [score_], 'times': [(start+stop)/2], 'epoch' : [epoch], \n",
    "                                      'variable' : [variable+'_'+str(hit)]})\n",
    "            df_iter = pd.concat([df_iter,new_row], ignore_index=True)\n",
    "        \n",
    "    times.append('trial_type')\n",
    "    real_score.append(variable+'_'+str(hit))\n",
    "    a_series = pd.DataFrame([real_score], columns = times)\n",
    "\n",
    "    df_real = pd.concat([df_real,a_series], ignore_index=True)\n",
    "    \n",
    "    return df_real, df_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "os.chdir(path_to_data)\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "# for filename in list_of_sessions:\n",
    "    # \n",
    "    if filename[-3:] != 'pdf':\n",
    "        df = pd.read_csv(filename, sep=',',index_col=0)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    print(filename, '/ Total session trials: ', len(df.trial.unique()), '/ Number of neurons: ', len(df.cluster_id.unique()))\n",
    "    \n",
    "    df = ephys.add_time_before_stimulus(df, 4)\n",
    "\n",
    "    substract = True\n",
    "    df['delay'] = np.around(df.delay,2)\n",
    "    # Variables used for decoder training\n",
    "    decode = 'vector_answer'\n",
    "    align='Delay_OFF'\n",
    "    ratio = 0.6\n",
    "    delay_train = 'all'\n",
    "    start = -0.5\n",
    "    stop = 0\n",
    "    type_trial ='WM_roll'\n",
    "    hit = 1\n",
    "    nsplits = 5\n",
    "    \n",
    "    cluster_list = df.cluster_id.unique()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=nsplits, shuffle=True)\n",
    "    # skf = KFold(n_splits=nsplits, shuffle=True)\n",
    "\n",
    "    # Create a dataframe for training data\n",
    "    if type_trial == 'all':\n",
    "        df_train = df.loc[(df.hit==hit)]\n",
    "    elif hit == 'all':\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)]\n",
    "    else:\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)&(df.hit==hit)]\n",
    "\n",
    "    if delay_train != 'all':\n",
    "        df_train = df_train.loc[(df_train.delay==delay_train)]\n",
    "    else:\n",
    "        df_train = df_train.loc[(df_train.delay!=0.2)&(df_train.delay!=0.1)]\n",
    "        \n",
    "    df_final, y = interval_extraction_trial(df_train, variable = decode, align = align, start = start, stop = stop, cluster_list=cluster_list)\n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.drop(columns ='trial')\n",
    "    \n",
    "    # Assuming df is your DataFrame with neural spike data\n",
    "    # Replace 'features' and 'target' with the actual column names\n",
    "    X = df.drop(columns='target')  # Features (neural spike data)\n",
    "    y = df['target']  # Target variable\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define a pipeline for scaling and logistic regression\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standardize features\n",
    "        ('logreg', LogisticRegression(max_iter=500))  # Logistic Regression\n",
    "    ])\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'logreg__solver': ['liblinear', 'saga'],  # Solvers to test\n",
    "        'logreg__penalty': ['l1', 'l2'],          # Regularization types\n",
    "        'logreg__C': [0.01, 0.1, 1, 10, 100]     # Regularization strength\n",
    "    }\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Fit the grid search to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and corresponding score\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Cross-Validated Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    print(\"Test Set Accuracy:\", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_________________________________________\n",
    "#### **Train in one delay and test in other delays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dataframe used for cumulative analysis\n",
    "df_cum = pd.DataFrame()\n",
    "df_cum_shuffle = pd.DataFrame()\n",
    "\n",
    "os.chdir(path_to_data)\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "# for filename in list_of_sessions:\n",
    "    # \n",
    "    if filename[-3:] != 'pdf':\n",
    "        df = pd.read_csv(filename, sep=',',index_col=0)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    print(filename, '/ Total session trials: ', len(df.trial.unique()), '/ Number of neurons: ', len(df.cluster_id.unique()))\n",
    "    \n",
    "    df = ephys.add_time_before_stimulus(df, 4)\n",
    "\n",
    "    substract = True\n",
    "    df['delay'] = np.around(df.delay,2)\n",
    "    \n",
    "    # Variables used for decoder training\n",
    "    decode = 'vector_answer'\n",
    "    align='Delay_OFF'\n",
    "    ratio = 0.6\n",
    "    delay_train = 'all'\n",
    "    start = -0.5\n",
    "    stop = 0\n",
    "    type_trial ='WM_roll'\n",
    "    hit = 1\n",
    "    nsplits = 5\n",
    "    \n",
    "    #Variables for testing\n",
    "    colors=['darkgreen','orangered']\n",
    "    variables = ['WM_roll','WM_roll']\n",
    "    hits = [1,1]\n",
    "    ratios = [0.6,0.6]\n",
    "    delays = [3, 10]\n",
    "    variables_combined=[variables[0]+'_'+str(hits[0]),variables[1]+'_'+str(hits[1])]\n",
    "\n",
    "    # colors=['crimson','darkgreen','indigo','purple']\n",
    "    # variables = ['WM_roll','WM_roll','RL_roll','RL_roll']\n",
    "    # hits = [0,1,1,0]\n",
    "    # ratios = [0.6,0.6,0.4,0.4]\n",
    "    # variables_combined=[variables[0]+'_'+str(hits[0]),variables[1]+'_'+str(hits[1]),variables[2]+'_'+str(hits[2]),variables[3]+'_'+str(hits[3])]\n",
    "\n",
    "    if decode == 'previous_vector_answer':\n",
    "        # This is only for doing the previous choice\n",
    "        df['after_correct'] = np.where(\n",
    "            df['previous_vector_answer'] == df['previous_reward_side'], 1, 0)\n",
    "        df = df.loc[df.after_correct == 1]\n",
    "    elif decode == 'drive':\n",
    "        if align == 'Delay_OFF':\n",
    "            df['drive'] = np.where((df.times>-df.delay)&(df.times< 0), 1, 0)\n",
    "        elif align == 'Stimulus_ON':\n",
    "            df['drive'] = np.where((df.times>0)&(df.times< df.delay+0.4), 1, 0)\n",
    "            \n",
    "    cluster_list = df.cluster_id.unique()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=nsplits, shuffle=True)\n",
    "    # skf = KFold(n_splits=nsplits, shuffle=True)\n",
    "\n",
    "    # Create a dataframe for training data\n",
    "    if type_trial == 'all':\n",
    "        df_train = df.loc[(df.hit==hit)]\n",
    "    elif hit == 'all':\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)]\n",
    "    else:\n",
    "        df_train = df.loc[(df[type_trial]>=ratio)&(df.hit==hit)]\n",
    "\n",
    "    if delay_train != 'all':\n",
    "        df_train = df_train.loc[(df_train.delay==delay_train)]\n",
    "    else:\n",
    "        df_train = df_train.loc[(df_train.delay!=0.2)&(df_train.delay!=0.1)]\n",
    "            \n",
    "    df_final, y = interval_extraction_trial(df_train, variable = decode, align = align, start = start, stop = stop, cluster_list=cluster_list)\n",
    "    df_final.reset_index(inplace=True)\n",
    "    df_final = df_final.drop(columns ='trial')\n",
    "\n",
    "    fold_no = 1\n",
    "    if len(y) < nsplits:\n",
    "        print('Skip session because not enough trials')\n",
    "        continue\n",
    "        \n",
    "    for train_index, test_index in skf.split(df_final, y):\n",
    "        print('Fold_no:', fold_no)\n",
    "        model, train_cols, score, sc_fit = train(df_train, decode=decode, align=align, start=start,stop=stop, cluster_list = cluster_list, \n",
    "                                  test_index=test_index,  train_index=train_index)\n",
    "\n",
    "        # Remove a fifth of the dataset so it can be compared to crossvalidated data. If we want to randomly reduce it, add reduce to trian function\n",
    "        # drop_list = np.array_split(df_train.trial.unique(), 5)[fold_no]\n",
    "        # df_train = df_train[~df_train['trial'].isin(drop_list)]\n",
    "        # index_train_trials = df_train.trial.unique()\n",
    "        # print('Total of left: ', len(df_train.loc[df_train['vector_answer'] == 0].groupby('trial').mean()), '; Total of right: ', len(df_train.loc[df_train['vector_answer'] == 1].groupby('trial').mean()))\n",
    "\n",
    "        fig, ax1 = plt.subplots(1,1, figsize=(8, 4), sharey=True)\n",
    "\n",
    "        for color, variable, delay, hit, ratio, left in zip(colors,variables, delays, hits,ratios,[ax1,ax1,ax1,ax1]):\n",
    "            df_res = pd.DataFrame()\n",
    "            df_sti = pd.DataFrame()\n",
    "            df_iter = pd.DataFrame()\n",
    "            try:\n",
    "                df_delay = df.loc[np.around(df.delay,1)==delay]\n",
    "                delay=np.around(df_delay.delay.iloc[0],1)\n",
    "                print('Delay:', delay)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if delay == 0.1 or delay == 0.2:\n",
    "                endrange=3.5\n",
    "                r=0.25\n",
    "\n",
    "            elif delay == 1:\n",
    "                endrange=4.5\n",
    "                r=0.25\n",
    "\n",
    "            elif delay == 3:\n",
    "                endrange=6.5    \n",
    "                r=0.25\n",
    "                \n",
    "            elif delay == 10:\n",
    "                endrange=14.5\n",
    "                r=0.25\n",
    "            \n",
    "            # Create a dataframe for testing data\n",
    "            if variable == 'all':\n",
    "                df_test = df_delay.loc[(df_delay.hit==hit)]\n",
    "            elif hit == 'all':\n",
    "                df_test = df_delay.loc[(df_delay[variable]>=ratio)]\n",
    "            else:\n",
    "                df_test = df_delay.loc[(df_delay[variable]>=ratio)&(df_delay.hit==hit)]\n",
    "                \n",
    "            if fold_no == 1:\n",
    "                print(delay, variable, 'Threshold:', ratio, 'Hit:', hit, 'Nº of trials:', len(df_test.trial.unique()))\n",
    "\n",
    "# -----------  Remove the trials that overlap with the training set.\n",
    "            list_train_trials = df_train.trial.unique()[train_index]\n",
    "            df_test = df_test[~df_test['trial'].isin(list_train_trials)] \n",
    "            \n",
    "            if len(df_test.trial.unique())<5:\n",
    "                print('Not enough trials with this condition')\n",
    "                continue\n",
    "\n",
    "            df_real,df_temp = test(df_test, sc_fit, decode= decode,epoch='Stimulus_ON',initrange=-2,endrange=endrange, r=r, model = model, delay_only=delay, variable=variable, hit=hit, nsurrogates = 100,train_cols = train_cols, cluster_list = cluster_list)\n",
    "\n",
    "            df_sti = pd.concat([df_real,df_sti])\n",
    "            df_iter = pd.concat([df_iter,df_temp])\n",
    "            \n",
    "            variable = str(variable)+'_'+str(hit)\n",
    "\n",
    "            # Aligmnent for Stimulus cue\n",
    "            real = df_sti.loc[(df_sti['trial_type'] ==variable)].to_numpy()\n",
    "            times = np.around(np.array(df_sti.columns)[:-1].astype(float),2)\n",
    "\n",
    "            df_new= df_iter.loc[(df_iter.epoch=='Stimulus_ON')].groupby('times')['score']\n",
    "            y_mean= df_new.mean().values\n",
    "            lower =  df_new.quantile(q=0.975, interpolation='linear') - y_mean\n",
    "            upper =  df_new.quantile(q=0.025, interpolation='linear') - y_mean\n",
    "            x=times\n",
    "            \n",
    "            ephys.plot_results_decoder(fig, real[0][:len(y_mean)], times, df_new,  ax1, color = color, epoch = 'Delay_OFF', \n",
    "                        y_range = [-0.05, 0.5], x_range = None, substract=True)\n",
    "            \n",
    "            df_cum, df_cum_shuffle = ephys.add_to_summary(real[0][:len(y_mean)], y_mean, times, filename, variable, epoch = 'Stimulus_ON', fold_no=fold_no, df_iter=df_iter, df_cum=df_cum, df_cum_shuffle=df_cum_shuffle, substract=False, delay=delay)\n",
    "            \n",
    "            sns.despine()\n",
    "            \n",
    "        fold_no+=1\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cum['trial_type'] = np.where(df_cum['delay'] == 3, 'WM_3', 'WM_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8, 4), sharey=True)\n",
    "\n",
    "ephys.plot_results_session_summary(ax, df_cum, colors, variables_combined = ['WM_3', 'WM_10'], \n",
    "                                 y_range = [0.4, 0.9], x_range = [-2, 14], epoch = 'Stimulus_ON', baseline=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = df_cum.groupby('session').delay.mean()\n",
    "pl = pl.reset_index()\n",
    "\n",
    "pl['all_trials']  = np.where(pl['delay'] == 10, False, True)\n",
    "list_sessions = pl.loc[pl.all_trials == True].session.unique()\n",
    "\n",
    "df_cum = df_cum.loc[df_cum.session.isin(list_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10, 4), sharey=True)\n",
    "ephys.plot_results_session_summary_substract(fig, ax, df_cum, df_cum_shuffle, color='darkgreen', variable='WM_roll_1', y_range = [-0.05, 0.3], x_range = None, epoch = 'Stimulus_ON', baseline=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = 'G:/My Drive/WORKING_MEMORY/PAPER/ANALYSIS_figures/'\n",
    "# os.chdir(save_path)\n",
    "\n",
    "file_name = 'review_crossdelay_decoder_train3'\n",
    "df_cum.to_csv(file_name+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
