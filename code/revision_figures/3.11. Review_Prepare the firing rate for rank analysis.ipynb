{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython magig  tools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from neo.core import SpikeTrain\n",
    "from quantities import ms, s\n",
    "from elephant.statistics import mean_firing_rate\n",
    "from elephant.statistics import time_histogram\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"G:/My Drive/WORKING_MEMORY/EXPERIMENTS/ELECTROPHYSIOLOGY/ANALYSIS/src/functions/\")\n",
    "import ephys_functions as ephys\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings. filterwarnings('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the synch data from the paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"G:/My Drive/WORKING_MEMORY/PAPER/ANALYSIS_Figures/instantaneous firing rate_alltrial_5ms.csv\", index_col=0)\n",
    "df_final = pd.read_csv(\"G:/My Drive/WORKING_MEMORY/PAPER/WM_manuscript_FIGURES/Fig. 7. Synch/synch_data_trials_2beforeSti.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = \"E22_2022-01-13_16-34-24.csv\"\n",
    "for animal in df_final.animal.unique():\n",
    "    # file_name = 'synch_data_trials_2beforeSti'\n",
    "    # df_final = pd.read_csv(file_name+'.csv', index_col=0)\n",
    "    threshold = 0.5\n",
    "\n",
    "    df_session = df_final.loc[df_final.animal==animal]\n",
    "\n",
    "    fig, ax = plt.subplots(2,1 , figsize=(10,6))\n",
    "    panel = ax[0]\n",
    "    panel1 = ax[1]\n",
    "\n",
    "\n",
    "    sns.lineplot(x=\"trial\", y=\"synch_window\",data=df_session, color='black',ci=68,ax=panel)      \n",
    "    sns.lineplot(x=\"trial\", y=\"synch\",data=df_session, color='grey',ci=68,ax=panel1)      \n",
    "    for plot in [panel, panel1]:\n",
    "        plot.set_ylabel('Synch')\n",
    "        plot.set_ylim(0.9,max(df_session.synch_window)+0.3)\n",
    "        plot.fill_between(df_session['trial'],0.9 , 2.5, where=df_session['WM_roll'] <= threshold,\n",
    "                    facecolor='indigo', alpha=0.3)\n",
    "        plot.fill_between(df_session['trial'], 0.9, 2.5,  where=df_session['WM_roll'] >= threshold,\n",
    "                        facecolor='darkgreen', alpha=0.3)\n",
    "    panel.set_xlabel('Trials')\n",
    "    panel.set_title('Mouse E22 13-01')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-do to obtain activity per bin for all neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to sessions\n",
    "path = r'E:\\Ephys\\summary_complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables we want to recover\n",
    "bin_size = 0.02\n",
    "concat_df = pd.DataFrame()\n",
    "align = 'Stimulus_ON'\n",
    "for filename in os.listdir(path):\n",
    "    print(filename)   \n",
    "\n",
    "    if filename[-3:] != 'pdf':\n",
    "        df = pd.read_csv(path + \"/\"+ filename, sep=',',index_col=0)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Add time before stimulus from the previous trial\n",
    "    df = ephys.add_time_before_stimulus(df, 4)\n",
    "    \n",
    "    for align, start, stop, delay_epoch in zip([align],[-2],\n",
    "                                               [0],[False]):\n",
    "\n",
    "        # Sort per trial\n",
    "        df = df.sort_values('trial')\n",
    "        # Get unique neurons in the whole session\n",
    "        unique_neurons = df.cluster_id.unique()\n",
    "        \n",
    "        #Create new aligment to the end of the session\n",
    "        df['a_'+align] = df.fixed_times-df[align]\n",
    "\n",
    "        # Define bin edges (e.g., 20 ms bins up to max time)\n",
    "        bins = np.arange(-2, 0 + bin_size, bin_size)\n",
    "        bin_labels = bins[:-1]  # Labels for bins (start times)\n",
    "\n",
    "        # Assign each spike to a time bin\n",
    "        df['a_'+align + '_bin'] = pd.cut(df['a_' + align], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "        \n",
    "        # Skip sessions with less than 30 neurons\n",
    "        total_neurons_session = len(df.cluster_id.unique())\n",
    "        if total_neurons_session < 30:\n",
    "            continue\n",
    "            \n",
    "        for T in df.trial.unique(): \n",
    "            # Select the trial that we want to look at this time\n",
    "            dft = df.loc[df.trial ==T]\n",
    "            \n",
    "            # Create a DataFrame with all possible combinations of cluster_id and a_END_bin\n",
    "            all_combinations = pd.MultiIndex.from_product([unique_neurons, bin_labels], names=['cluster_id', 'a_' + align + '_bin']).to_frame(index=False)\n",
    "        \n",
    "            # Merge with the actual data to fill missing combinations with NaNs\n",
    "            binned_spikes = pd.merge(all_combinations, dft.groupby(['cluster_id', 'a_' + align + '_bin']).size().reset_index(name='spike_count'), on=['cluster_id', 'a_' + align + '_bin'], how='left')\n",
    "\n",
    "            # Fill NaNs in spike_count with 0\n",
    "            binned_spikes['spike_count'].fillna(0, inplace=True)\n",
    "\n",
    "            # Convert 'time_bin' back to numeric\n",
    "            binned_spikes['a_' + align + '_bin'] = binned_spikes['a_' + align + '_bin'].astype(float)\n",
    "            binned_spikes['trial'] = T\n",
    "            binned_spikes['session'] = filename\n",
    "            binned_spikes['WM_roll'] = dft.WM_roll.unique()[0]\n",
    "            binned_spikes['hit'] = dft.hit.unique()[0]\n",
    "            concat_df = pd.concat([concat_df, binned_spikes], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['WM'] = np.where(concat_df['WM_roll'] > 0.5, 1, 0)\n",
    "concat_df.drop(columns=['WM_roll'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_df.drop_duplicates(subset=['session', 'cluster_id', 'trial', 'WM'])\n",
    "grp1 = df.groupby(['session', 'WM', 'cluster_id']).trial.nunique().reset_index()\n",
    "grp2 = grp1.groupby(['session', 'WM']).agg(mean_trial = ('trial', 'mean'), std_trial = ('trial', 'std')).reset_index()\n",
    "grp2.std_trial.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.to_csv(r'C:\\Users\\tiffany.ona\\Documents\\working_memory\\data\\3_11_Review_Prepare the firing rate for rank analysis\\20ms_rank_df_fixed_V2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chekc why the two datasets don't have the same number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"G:/My Drive/WORKING_MEMORY/PAPER/WM_manuscript_FIGURES/Fig. 7. Synch/synch_data_trials_2beforeSti.csv\", index_col=0)\n",
    "df_final['state'] = np.where(df_final['WM_roll']>0.5, 0, 1)\n",
    "concat_df = pd.read_csv(r'C:\\Users\\tiffany.ona\\Documents\\working_memory\\data\\3_11_Review_Prepare the firing rate for rank analysis\\20ms_rank_df_fixed_V2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reobtain the synch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_window_centered(data, runningwindow,option, trials='trials'):\n",
    "    \"\"\"\n",
    "    Computes a rolling average with a length of runningwindow samples.\n",
    "    \"\"\"\n",
    "    performance = []\n",
    "    start_on=False\n",
    "    for i in range(len(data)):\n",
    "        if data[trials].iloc[i] < int(runningwindow/2):\n",
    "            # Store the first index of that session for the first initial trials\n",
    "            if start_on == False:\n",
    "                start=i\n",
    "                start_on=True\n",
    "            performance.append(round(np.mean(data[option].iloc[start:i + int(runningwindow/2)]), 3))\n",
    "        elif i < (len(data)-runningwindow):\n",
    "            if data[trials].iloc[i] > data[trials].iloc[i+runningwindow]:\n",
    "                # Store the last values for the end of the session\n",
    "                if end == True:\n",
    "                    end_value = i+runningwindow-1\n",
    "                    end = False\n",
    "                performance.append(round(np.mean(data[option].iloc[i:end_value]), 3))\n",
    "                \n",
    "            else: # Rest of the session\n",
    "                start_on=False\n",
    "                end = True\n",
    "                performance.append(round(np.mean(data[option].iloc[i - int(runningwindow/2):i+int(runningwindow/2)]), 3))\n",
    "            \n",
    "        else:\n",
    "            performance.append(round(np.mean(data[option].iloc[i:len(data)]), 3))\n",
    "\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables we want to recover\n",
    "mean_firing= []\n",
    "cv=[]\n",
    "synch = []\n",
    "repeat_list=[]\n",
    "hit_list=[]\n",
    "wm_list=[]\n",
    "animal_list = []\n",
    "T_max = []\n",
    "trial = []\n",
    "synch_mean_window=[]\n",
    "aligment_window=[]\n",
    "\n",
    "surrogates=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(path):\n",
    "    print(filename)   \n",
    "\n",
    "    if filename[-3:] != 'pdf':\n",
    "        df = pd.read_csv(path + \"/\"+ filename, sep=',',index_col=0)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    print(filename)   \n",
    "    \n",
    "    # Add time before stimulus from the previous trial\n",
    "    df = ephys.add_time_before_stimulus(df, 4)\n",
    "    \n",
    "#################################################### --------------------------------------------------------------------\n",
    "    # for align, start, stop, delay_epoch in zip(['Stimulus_ON','Delay_OFF','Delay_OFF','Lick_ON'],[-2,-2,0,-0.25],\n",
    "    #                                            [0,0,2,1.5],[False, True, False, False]):\n",
    "    for align, start, stop, delay_epoch in zip(['Stimulus_ON'],[-2],\n",
    "                                               [0],[False]):\n",
    "\n",
    "        #Create new aligment to the end of the session\n",
    "        df['a_'+align] = df.fixed_times-df[align]\n",
    "\n",
    "        # Sort per trial\n",
    "        df = df.sort_values('trial')\n",
    "\n",
    "        #Filter for the last 2 seconds of the ITI\n",
    "        # df = df.loc[(df['a_'+align]>start)&(df['a_'+align]<stop)]\n",
    "\n",
    "        # total_neurons_session = len(df.loc[(df.trial_start<df.trial)&(df.trial_end>df.trial)].cluster_id.unique())\n",
    "        total_neurons_session = len(df.cluster_id.unique())\n",
    "        if total_neurons_session < 30:\n",
    "            continue\n",
    "            \n",
    "        print('There are ' + str(total_neurons_session) + ' neurons')\n",
    "\n",
    "        for T in df.trial.unique(): \n",
    "            # Select the trial that we want to look at this time\n",
    "            dft = df.loc[df.trial ==T]\n",
    "                \n",
    "            dft = dft.loc[(dft['a_'+align]>start)&(dft['a_'+align]<stop)]\n",
    "\n",
    "            # Recover amount of neurons that were being registered at that trial interval\n",
    "    #         n_neurons = len(df.loc[(df.trial_start<T)&(df.trial_end>T)].cluster_id.unique())\n",
    "            n_neurons = len(df.cluster_id.unique())\n",
    "\n",
    "            hit_list.append(dft.hit.unique()[0])\n",
    "            repeat_list.append(dft.repeat_choice.unique()[0])\n",
    "            wm_list.append(dft.WM_roll.unique()[0])\n",
    "\n",
    "            if delay_epoch:\n",
    "                # if dft.delay.unique()[0] == 0.1 or dft.delay.unique()[0] == 0.2:\n",
    "                if dft.delay.unique()[0] != 1 and dft.delay.unique()[0] == 3 and dft.delay.unique()[0] == 3:\n",
    "                    mean_firing.append(np.nan)\n",
    "                    synch.append(np.nan)\n",
    "                    trial.append(T)\n",
    "                    animal_list.append(filename)\n",
    "                    aligment_window.append(align+'_'+str(start)+'_'+str(stop))\n",
    "                    T_max.append(max(df.trial.unique()))\n",
    "                    continue\n",
    "                    \n",
    "            times_spikes = dft['a_'+align].values\n",
    "            times_spikes = times_spikes*1000*ms #transform to ms\n",
    "\n",
    "            ############################################################ Set the strat and end time of the train\n",
    "            stop_time =  stop*1000*ms ## End of the trial in ms\n",
    "            start_time = start*1000*ms ## Start of the trial in ms     \n",
    "\n",
    "            ############################################################ Spiketrain\n",
    "            spiketrain = SpikeTrain(times_spikes, units=ms, t_stop=stop_time, t_start=start_time) \n",
    "\n",
    "            ############################################################ \n",
    "            histogram_rate = time_histogram([spiketrain], 20*ms, output='rate')\n",
    "            times_ = histogram_rate.times.rescale(s)\n",
    "            firing_real = histogram_rate.rescale(histogram_rate.dimensionality).magnitude.flatten()\n",
    "\n",
    "            mean_firing.append(float(mean_firing_rate(spiketrain)/n_neurons*1000))\n",
    "    #         print(\"The mean firing rate of spiketrain is\", mean_firing_rate(spiketrain)/n_neurons)\n",
    "\n",
    "    #         print(\"The std of the firing rate of spiketrain is\", np.std(firing))\n",
    "            real_std=np.std(firing_real) # Store the real std value\n",
    "\n",
    "            # t1_start = process_time() \n",
    "            list_std = []\n",
    "            for i in range(surrogates):\n",
    "                # Create a random shuffle for the same amount of spikes in that interval\n",
    "                random_float_list = np.random.uniform(start, stop, len(times_spikes))\n",
    "                # t1_stop = process_time() \n",
    "                # print(str(T) + ' ' + str(t1_stop-t1_start))\n",
    "                \n",
    "                surrogate_spikes = np.array(random_float_list)*1000*ms #transform to ms\n",
    "                spiketrain = SpikeTrain(surrogate_spikes, units=ms, t_stop=stop_time, t_start=start_time) \n",
    "\n",
    "                histogram_rate = time_histogram([spiketrain], 20*ms, output='rate')\n",
    "                times_ = histogram_rate.times.rescale(s)\n",
    "                firing = histogram_rate.rescale(histogram_rate.dimensionality).magnitude.flatten()\n",
    "\n",
    "                list_std.append(np.std(firing))\n",
    "\n",
    "            synch.append(real_std/np.mean(list_std))\n",
    "            \n",
    "            trial.append(T)\n",
    "            animal_list.append(filename)\n",
    "            aligment_window.append(align+'_'+str(start)+'_'+str(stop))\n",
    "            T_max.append(max(df.trial.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6,2 , figsize=(12,20))\n",
    "for session, ax in zip(df_final.animal.unique(), axes.flatten()):\n",
    "    sns.lineplot(x=\"trials\", y=\"FR\",data=df_final.loc[df_final.animal == session],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with all the recovered lists. \n",
    "df_final = pd.DataFrame(list(zip(mean_firing, synch, hit_list, wm_list, repeat_list, T_max,trial,animal_list, aligment_window)), \n",
    "       columns =['FR', 'synch','hit','WM_roll','repeat','total_trials','trials','animal', 'aligment'])\n",
    "\n",
    "df_final.dropna(inplace=True)\n",
    "\n",
    "df_final['accuracy'] = compute_window_centered(df_final,10,'hit')\n",
    "df_final['repeat_bias'] = compute_window_centered(df_final,10,'repeat')\n",
    "df_final['synch_window']= compute_window_centered(df_final,5,'synch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
