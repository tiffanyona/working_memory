{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython magig  tools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from neo.core import SpikeTrain\n",
    "from quantities import ms, s, Hz\n",
    "from elephant.statistics import mean_firing_rate\n",
    "from elephant.statistics import time_histogram, instantaneous_rate\n",
    "from elephant.kernels import GaussianKernel\n",
    "from elephant.statistics import mean_firing_rate\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"G:/My Drive/WORKING_MEMORY/EXPERIMENTS/ELECTROPHYSIOLOGY/ANALYSIS/src/functions/\")\n",
    "import ephys_functions as ephys\n",
    "import model_functions as mod\n",
    "import behavioral_functions as beh\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings. filterwarnings('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the synch data from the paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"G:/My Drive/WORKING_MEMORY/PAPER/ANALYSIS_Figures/instantaneous firing rate_alltrial_5ms.csv\", index_col=0)\n",
    "df_final = pd.read_csv(\"G:/My Drive/WORKING_MEMORY/PAPER/WM_manuscript_FIGURES/Fig. 7. Synch/synch_data_trials_2beforeSti.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = \"E22_2022-01-13_16-34-24.csv\"\n",
    "for animal in df_final.animal.unique():\n",
    "    # file_name = 'synch_data_trials_2beforeSti'\n",
    "    # df_final = pd.read_csv(file_name+'.csv', index_col=0)\n",
    "    threshold = 0.5\n",
    "\n",
    "    df_session = df_final.loc[df_final.animal==animal]\n",
    "\n",
    "    fig, ax = plt.subplots(2,1 , figsize=(10,6))\n",
    "    panel = ax[0]\n",
    "    panel1 = ax[1]\n",
    "\n",
    "\n",
    "    sns.lineplot(x=\"trial\", y=\"synch_window\",data=df_session, color='black',ci=68,ax=panel)      \n",
    "    sns.lineplot(x=\"trial\", y=\"synch\",data=df_session, color='grey',ci=68,ax=panel1)      \n",
    "    for plot in [panel, panel1]:\n",
    "        plot.set_ylabel('Synch')\n",
    "        plot.set_ylim(0.9,max(df_session.synch_window)+0.3)\n",
    "        plot.fill_between(df_session['trial'],0.9 , 2.5, where=df_session['WM_roll'] <= threshold,\n",
    "                    facecolor='indigo', alpha=0.3)\n",
    "        plot.fill_between(df_session['trial'], 0.9, 2.5,  where=df_session['WM_roll'] >= threshold,\n",
    "                        facecolor='darkgreen', alpha=0.3)\n",
    "    panel.set_xlabel('Trials')\n",
    "    panel.set_title('Mouse E22 13-01')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-do to obtain activity per bin for all neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to sessions\n",
    "path = r'E:\\Ephys\\summary_complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E17_2022-02-13_17-14-28complete_graph.pdf\n",
      "E19_2022-01-14_14-42-13.csv\n",
      "E19_2022-01-14_14-42-13complete_graph.pdf\n",
      "E19_2022-01-15_15-13-28.csv\n",
      "E19_2022-01-15_15-13-28complete_graph.pdf\n",
      "E19_2022-01-16_15-40-47.csv\n",
      "E19_2022-01-16_15-40-47complete_graph.pdf\n",
      "E19_2022-01-17_15-34-37.csv\n",
      "E19_2022-01-17_15-34-37complete_graph.pdf\n",
      "E20_2022-02-12_15-23-55.csv\n",
      "E20_2022-02-12_15-23-55complete_graph.pdf\n",
      "E20_2022-02-13_15-10-51.csv\n",
      "E20_2022-02-13_15-10-51complete_graph.pdf\n",
      "E20_2022-02-14_16-01-30.csv\n",
      "E20_2022-02-14_16-01-30complete_graph.pdf\n",
      "E20_2022-02-15_16-02-28.csv\n",
      "E20_2022-02-15_16-02-28complete_graph.pdf\n",
      "E20_2022-02-26_16-49-05.csv\n",
      "E20_2022-02-26_16-49-05complete_graph.pdf\n",
      "E20_2022-02-27_17-02-17.csv\n",
      "E20_2022-02-27_17-02-17complete_graph.pdf\n",
      "E20_2022-02-28_16-05-49.csv\n",
      "E20_2022-02-28_16-05-49complete_graph.pdf\n",
      "E20_2022-03-01_16-11-01.csv\n",
      "E20_2022-03-01_16-11-01complete_graph.pdf\n",
      "E22_2022-01-13_16-34-24.csv\n",
      "E22_2022-01-13_16-34-24complete_graph.pdf\n",
      "E22_2022-01-14_16-50-37.csv\n",
      "E22_2022-01-14_16-50-37complete_graph.pdf\n",
      "E22_2022-01-15_16-53-52.csv\n",
      "E22_2022-01-15_16-53-52complete_graph.pdf\n",
      "E22_2022-01-16_18-00-47.csv\n",
      "E22_2022-01-16_18-00-47complete_graph.pdf\n",
      "E22_2022-01-17_18-05-16.csv\n",
      "E22_2022-01-17_18-05-16complete_graph.pdf\n",
      "E22_2022-01-21_16-23-50.csv\n",
      "E22_2022-01-21_16-23-50complete_graph.pdf\n",
      "E22_2022-01-22_17-09-15.csv\n",
      "E22_2022-01-22_17-09-15complete_graph.pdf\n"
     ]
    }
   ],
   "source": [
    "# List of variables we want to recover\n",
    "bin_size = 0.02\n",
    "concat_df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    print(filename)   \n",
    "\n",
    "    if filename[-3:] != 'pdf':\n",
    "        df = pd.read_csv(path + \"/\"+ filename, sep=',',index_col=0)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Add time before stimulus from the previous trial\n",
    "    df = ephys.add_time_before_stimulus(df, 4)\n",
    "    \n",
    "    for align, start, stop, delay_epoch in zip(['END'],[-2],\n",
    "                                               [0],[False]):\n",
    "\n",
    "        # Sort per trial\n",
    "        df = df.sort_values('trial')\n",
    "        # Get unique neurons in the whole session\n",
    "        unique_neurons = df.cluster_id.unique()\n",
    "        \n",
    "        #Create new aligment to the end of the session\n",
    "        df['a_'+align] = df.fixed_times-df[align]\n",
    "\n",
    "        # Define bin edges (e.g., 20 ms bins up to max time)\n",
    "        bins = np.arange(-2, 0 + bin_size, bin_size)\n",
    "        bin_labels = bins[:-1]  # Labels for bins (start times)\n",
    "\n",
    "        # Assign each spike to a time bin\n",
    "        df['a_END_bin'] = pd.cut(df['a_END'], bins=bins, labels=bin_labels, include_lowest=True)\n",
    "        \n",
    "        # Skip sessions with less than 30 neurons\n",
    "        total_neurons_session = len(df.cluster_id.unique())\n",
    "        if total_neurons_session < 30:\n",
    "            continue\n",
    "            \n",
    "        for T in df.trial.unique(): \n",
    "            # Select the trial that we want to look at this time\n",
    "            dft = df.loc[df.trial ==T]\n",
    "            \n",
    "            # Create a DataFrame with all possible combinations of cluster_id and a_END_bin\n",
    "            all_combinations = pd.MultiIndex.from_product([unique_neurons, bin_labels], names=['cluster_id', 'a_END_bin']).to_frame(index=False)\n",
    "        \n",
    "            # Merge with the actual data to fill missing combinations with NaNs\n",
    "            binned_spikes = pd.merge(all_combinations, dft.groupby(['cluster_id', 'a_END_bin']).size().reset_index(name='spike_count'), on=['cluster_id', 'a_END_bin'], how='left')\n",
    "\n",
    "            # Fill NaNs in spike_count with 0\n",
    "            binned_spikes['spike_count'].fillna(0, inplace=True)\n",
    "\n",
    "            # Convert 'time_bin' back to numeric\n",
    "            binned_spikes['a_END_bin'] = binned_spikes['a_END_bin'].astype(float)\n",
    "            binned_spikes['trial'] = T\n",
    "            binned_spikes['session'] = filename\n",
    "            binned_spikes['WM_roll'] = dft.WM_roll.unique()[0]\n",
    "            concat_df = pd.concat([concat_df, binned_spikes], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['WM'] = np.where(concat_df['WM_roll'] > 0.5, 1, 0)\n",
    "concat_df.drop(columns=['WM_roll'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = concat_df.drop_duplicates(subset=['session', 'cluster_id', 'trial', 'WM'])\n",
    "grp1 = df.groupby(['session', 'WM', 'cluster_id']).trial.nunique().reset_index()\n",
    "grp2 = grp1.groupby(['session', 'WM']).agg(mean_trial = ('trial', 'mean'), std_trial = ('trial', 'std')).reset_index()\n",
    "grp2.std_trial.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.to_csv(r'C:\\Users\\tiffany.ona\\Documents\\working_memory\\data\\3_11_Review_Prepare the firing rate for rank analysis\\20ms_rank_df_fixed_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_df = pd.read_csv(r'C:\\Users\\tiffany.ona\\Documents\\working_memory\\data\\3_11_Review_Prepare the firing rate for rank analysis\\20ms_rank_df_fixed.csv', index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
